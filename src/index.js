import { createYoga, createSchema } from 'graphql-yoga'

// GraphQL Schema
const schema = createSchema({
  typeDefs: `
    type Query {
      hello: String
      models: [AIModel!]!
      debug: DebugInfo
    }

    type Mutation {
      sendMessage(input: ChatInput!): ChatResponse!
    }

    type AIModel {
      id: String!
      name: String!
      provider: String!
      description: String
    }

    type DebugInfo {
      hasOpenAI: Boolean!
      hasDeepSeek: Boolean!
      openaiLength: Int!
      deepseekLength: Int!
      envKeys: [String!]!
    }

    input ChatInput {
      message: String!
      model: String!
      temperature: Float
      maxTokens: Int
    }

    type ChatResponse {
      success: Boolean!
      message: String
      reply: String
      error: String
      usage: Usage
    }

    type Usage {
      promptTokens: Int
      completionTokens: Int
      totalTokens: Int
    }
  `,
  resolvers: {
    Query: {
      hello: () => {
        console.log('üìû Hello query called')
        return 'Hello from ZY-YD AI API!'
      },
      models: () => {
        console.log('üìã Models query called')
        return AI_MODELS
      },
      debug: (_, __, context) => {
        const { env } = context
        const envKeys = Object.keys(env || {})
        
        console.log('üîç Debug info requested:', {
          hasOpenAI: !!env?.OPENAI_API_KEY,
          hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
          envKeys
        })
        
        return {
          hasOpenAI: !!env?.OPENAI_API_KEY,
          hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
          openaiLength: env?.OPENAI_API_KEY ? env.OPENAI_API_KEY.length : 0,
          deepseekLength: env?.DEEPSEEK_API_KEY ? env.DEEPSEEK_API_KEY.length : 0,
          envKeys
        }
      }
    },
    Mutation: {
      sendMessage: async (_, args, context) => {
        console.log('üöÄ SendMessage mutation called with args:', args)
        
        try {
          const { input } = args
          const { message, model, temperature, maxTokens } = input
          const { env } = context

          // Ê†πÊçÆÊ®°Âûã‰ºòÂåñÈªòËÆ§ÂèÇÊï∞
          const optimizedParams = getOptimizedParams(model, temperature, maxTokens)

          console.log('üìù Processing message:', {
            model,
            messageLength: message.length,
            temperature: optimizedParams.temperature,
            maxTokens: optimizedParams.maxTokens
          })

          // ËØ¶ÁªÜÁöÑÁéØÂ¢ÉÂèòÈáèÊ£ÄÊü•
          console.log('üîë Environment analysis:', {
            envType: typeof env,
            envKeys: Object.keys(env || {}),
            hasOpenAI: !!env?.OPENAI_API_KEY,
            hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
            openaiType: typeof env?.OPENAI_API_KEY,
            deepseekType: typeof env?.DEEPSEEK_API_KEY,
            openaiLength: env?.OPENAI_API_KEY ? env.OPENAI_API_KEY.length : 0,
            deepseekLength: env?.DEEPSEEK_API_KEY ? env.DEEPSEEK_API_KEY.length : 0
          })

          // Ê†πÊçÆÊ®°ÂûãÈÄâÊã© API
          const modelConfig = AI_MODELS.find(m => m.id === model)
          if (!modelConfig) {
            console.error('‚ùå Model not found:', model)
            return {
              success: false,
              message: message,
              reply: null,
              error: `‰∏çÊîØÊåÅÁöÑÊ®°Âûã: ${model}`,
              usage: null
            }
          }

          console.log('üì¶ Using model config:', modelConfig)

          let result
          if (modelConfig.provider === 'openai') {
            const apiKey = env?.OPENAI_API_KEY
            console.log('üîç OpenAI Key check:', {
              exists: !!apiKey,
              type: typeof apiKey,
              length: apiKey ? apiKey.length : 0,
              startsWithSk: apiKey ? apiKey.startsWith('sk-') : false
            })
            
            if (!apiKey) {
              return {
                success: false,
                message: message,
                reply: null,
                error: 'OpenAI API Key Êú™ÈÖçÁΩÆ„ÄÇËØ∑Ê£ÄÊü•WorkersÁéØÂ¢ÉÂèòÈáèOPENAI_API_KEY',
                usage: null
              }
            }
            result = await callOpenAI(message, model, apiKey, optimizedParams.temperature, optimizedParams.maxTokens)
          } else if (modelConfig.provider === 'deepseek') {
            const apiKey = env?.DEEPSEEK_API_KEY
            console.log('üîç DeepSeek Key check:', {
              exists: !!apiKey,
              type: typeof apiKey,
              length: apiKey ? apiKey.length : 0,
              startsWithSk: apiKey ? apiKey.startsWith('sk-') : false
            })
            
            if (!apiKey) {
              return {
                success: false,
                message: message,
                reply: null,
                error: 'DeepSeek API Key Êú™ÈÖçÁΩÆ„ÄÇËØ∑Ê£ÄÊü•WorkersÁéØÂ¢ÉÂèòÈáèDEEPSEEK_API_KEY',
                usage: null
              }
            }
            result = await callDeepSeek(message, model, apiKey, optimizedParams.temperature, optimizedParams.maxTokens)
          } else {
            console.error('‚ùå Unsupported provider:', modelConfig.provider)
            return {
              success: false,
              message: message,
              reply: null,
              error: `‰∏çÊîØÊåÅÁöÑÊèê‰æõÂïÜ: ${modelConfig.provider}`,
              usage: null
            }
          }

          console.log('üéâ API call completed successfully')
          return {
            success: true,
            message: message,
            reply: result.reply,
            error: null,
            usage: result.usage
          }
        } catch (error) {
          console.error('üí• SendMessage mutation error:', error)
          return {
            success: false,
            message: args.input.message,
            reply: null,
            error: error.message || 'ÊúçÂä°Âô®ÂÜÖÈÉ®ÈîôËØØ',
            usage: null
          }
        }
      }
    }
  }
})

// Ê†πÊçÆÊ®°Âûã‰ºòÂåñÂèÇÊï∞
function getOptimizedParams(model, temperature, maxTokens) {
  // DeepSeek ‰ºòÂåñÂèÇÊï∞
  if (model.includes('deepseek')) {
    return {
      temperature: temperature !== undefined ? temperature : 0.3, // Èôç‰ΩéÈöèÊú∫ÊÄßÔºåÊèêÈ´òÂìçÂ∫îÈÄüÂ∫¶
      maxTokens: maxTokens !== undefined ? maxTokens : 800 // ÂáèÂ∞ëËæìÂá∫ÈïøÂ∫¶
    }
  }
  
  // OpenAI ÈªòËÆ§ÂèÇÊï∞
  return {
    temperature: temperature !== undefined ? temperature : 0.7,
    maxTokens: maxTokens !== undefined ? maxTokens : 1000
  }
}

// AI Ê®°ÂûãÈÖçÁΩÆ
const AI_MODELS = [
  {
    id: 'gpt-3.5-turbo',
    name: 'GPT-3.5 Turbo',
    provider: 'openai',
    description: 'OpenAIÁöÑÂø´ÈÄüÂìçÂ∫îÊ®°Âûã'
  },
  {
    id: 'gpt-4',
    name: 'GPT-4',
    provider: 'openai', 
    description: 'OpenAIÁöÑÊúÄÂº∫Ê®°Âûã'
  },
  {
    id: 'deepseek-chat',
    name: 'DeepSeek Chat',
    provider: 'deepseek',
    description: 'DeepSeekÁöÑÂØπËØùÊ®°Âûã (Â∑≤‰ºòÂåñÈÄüÂ∫¶)'
  },
  {
    id: 'deepseek-coder',
    name: 'DeepSeek Coder',
    provider: 'deepseek',
    description: 'DeepSeekÁöÑ‰ª£Á†ÅÁîüÊàêÊ®°Âûã (Â∑≤‰ºòÂåñÈÄüÂ∫¶)'
  }
]

// OpenAI API Ë∞ÉÁî®
async function callOpenAI(message, model, apiKey, temperature = 0.7, maxTokens = 1000) {
  console.log('ü§ñ Calling OpenAI API:', { model, messageLength: message.length, temperature, maxTokens })
  
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: message }],
      temperature: temperature,
      max_tokens: maxTokens
    })
  })

  const data = await response.json()
  
  if (!response.ok) {
    console.error('‚ùå OpenAI API Error:', data)
    throw new Error(data.error?.message || `OpenAI API error: ${response.status}`)
  }

  console.log('‚úÖ OpenAI API Success')
  return {
    reply: data.choices[0].message.content,
    usage: {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens
    }
  }
}

// DeepSeek API Ë∞ÉÁî® - ‰ºòÂåñÁâàÊú¨
async function callDeepSeek(message, model, apiKey, temperature = 0.3, maxTokens = 800) {
  console.log('üß† Calling DeepSeek API (Optimized):', { 
    model, 
    messageLength: message.length, 
    temperature, 
    maxTokens 
  })
  
  // ‰ºòÂåñÁöÑËØ∑Ê±Ç‰Ωì
  const requestBody = {
    model: model,
    messages: [{ role: 'user', content: message }],
    temperature: temperature,
    max_tokens: maxTokens,
    // DeepSeek ÁâπÂÆö‰ºòÂåñÂèÇÊï∞
    top_p: 0.8, // ÊéßÂà∂ËæìÂá∫ÁöÑÂ§öÊ†∑ÊÄßÔºåËæÉ‰ΩéÂÄºÊèêÈ´òÈÄüÂ∫¶
    frequency_penalty: 0.1, // ÂáèÂ∞ëÈáçÂ§çÔºåÊèêÈ´òÊïàÁéá
    presence_penalty: 0.1, // ÈºìÂä±Êñ∞ËØùÈ¢òÔºåÈÅøÂÖçÂÜóÈïø
    stop: null // ÊòéÁ°ÆËÆæÁΩÆÂÅúÊ≠¢Êù°‰ª∂
  }

  console.log('üîß DeepSeek request params:', requestBody)
  
  const response = await fetch('https://api.deepseek.com/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(requestBody)
  })

  const data = await response.json()
  
  if (!response.ok) {
    console.error('‚ùå DeepSeek API Error:', data)
    throw new Error(data.error?.message || `DeepSeek API error: ${response.status}`)
  }

  console.log('‚úÖ DeepSeek API Success (Optimized)')
  return {
    reply: data.choices[0].message.content,
    usage: {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens
    }
  }
}

// ÂàõÂª∫ GraphQL Yoga ÂÆû‰æã
const yoga = createYoga({
  schema,
  context: async ({ request, env }) => {
    console.log('üåç Creating GraphQL context for:', request.method)
    console.log('üîß Environment passed to context:', {
      envType: typeof env,
      envKeys: Object.keys(env || {}),
      hasOpenAI: !!env?.OPENAI_API_KEY,
      hasDeepSeek: !!env?.DEEPSEEK_API_KEY
    })
    return { request, env }
  },
  cors: {
    origin: [
      'https://zy-yd-ai-tools.pages.dev',
      'https://2000zy.space',
      'http://localhost:3000'
    ],
    methods: ['GET', 'POST', 'OPTIONS'],
    allowedHeaders: ['Content-Type', 'Authorization', 'Accept'],
    credentials: false
  },
  graphiql: {
    title: 'ZY-YD AI API - GraphQL Playground',
    headerEditorEnabled: true
  }
})

export default {
  async fetch(request, env, ctx) {
    const url = new URL(request.url)
    console.log('üåê Workers request:', {
      method: request.method,
      pathname: url.pathname,
      origin: request.headers.get('origin')
    })
    
    console.log('üîß Environment at fetch level:', {
      envType: typeof env,
      envKeys: Object.keys(env || {}),
      hasOpenAI: !!env?.OPENAI_API_KEY,
      hasDeepSeek: !!env?.DEEPSEEK_API_KEY
    })
    
    try {
      const response = await yoga.fetch(request, { env, ctx })
      console.log('‚úÖ Workers response:', response.status)
      return response
    } catch (error) {
      console.error('üí• Workers error:', error)
      return new Response(
        JSON.stringify({ 
          error: 'Workers Internal Error', 
          message: error.message,
          stack: error.stack?.split('\n').slice(0, 5)
        }), 
        { 
          status: 500, 
          headers: { 
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization'
          } 
        }
      )
    }
  }
}