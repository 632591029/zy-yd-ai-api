import { createYoga, createSchema } from 'graphql-yoga'

// GraphQL Schema
const schema = createSchema({
  typeDefs: `
    type Query {
      hello: String
      models: [AIModel!]!
      debug: DebugInfo
    }

    type Mutation {
      sendMessage(input: ChatInput!): ChatResponse!
    }

    type AIModel {
      id: String!
      name: String!
      provider: String!
      description: String
    }

    type DebugInfo {
      hasOpenAI: Boolean!
      hasDeepSeek: Boolean!
      openaiLength: Int!
      deepseekLength: Int!
      envKeys: [String!]!
    }

    input ChatInput {
      message: String!
      model: String!
      temperature: Float
      maxTokens: Int
    }

    type ChatResponse {
      success: Boolean!
      message: String
      reply: String
      error: String
      usage: Usage
    }

    type Usage {
      promptTokens: Int
      completionTokens: Int
      totalTokens: Int
    }
  `,
  resolvers: {
    Query: {
      hello: () => {
        console.log('ğŸ“ Hello query called')
        return 'Hello from ZY-YD AI API!'
      },
      models: () => {
        console.log('ğŸ“‹ Models query called')
        return AI_MODELS
      },
      debug: (_, __, context) => {
        const { env } = context
        const envKeys = Object.keys(env || {})
        
        console.log('ğŸ” Debug info requested:', {
          hasOpenAI: !!env?.OPENAI_API_KEY,
          hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
          envKeys
        })
        
        return {
          hasOpenAI: !!env?.OPENAI_API_KEY,
          hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
          openaiLength: env?.OPENAI_API_KEY ? env.OPENAI_API_KEY.length : 0,
          deepseekLength: env?.DEEPSEEK_API_KEY ? env.DEEPSEEK_API_KEY.length : 0,
          envKeys
        }
      }
    },
    Mutation: {
      sendMessage: async (_, args, context) => {
        console.log('ğŸš€ SendMessage mutation called with args:', args)
        
        try {
          const { input } = args
          const { message, model, temperature, maxTokens } = input
          const { env } = context

          // æ ¹æ®æ¨¡å‹ä¼˜åŒ–é»˜è®¤å‚æ•° - å¹³è¡¡ç‰ˆæœ¬
          const optimizedParams = getOptimizedParams(model, temperature, maxTokens)

          console.log('ğŸ“ Processing message:', {
            model,
            messageLength: message.length,
            temperature: optimizedParams.temperature,
            maxTokens: optimizedParams.maxTokens
          })

          // è¯¦ç»†çš„ç¯å¢ƒå˜é‡æ£€æŸ¥
          console.log('ğŸ”‘ Environment analysis:', {
            envType: typeof env,
            envKeys: Object.keys(env || {}),
            hasOpenAI: !!env?.OPENAI_API_KEY,
            hasDeepSeek: !!env?.DEEPSEEK_API_KEY,
            openaiType: typeof env?.OPENAI_API_KEY,
            deepseekType: typeof env?.DEEPSEEK_API_KEY,
            openaiLength: env?.OPENAI_API_KEY ? env.OPENAI_API_KEY.length : 0,
            deepseekLength: env?.DEEPSEEK_API_KEY ? env.DEEPSEEK_API_KEY.length : 0
          })

          // æ ¹æ®æ¨¡å‹é€‰æ‹© API
          const modelConfig = AI_MODELS.find(m => m.id === model)
          if (!modelConfig) {
            console.error('âŒ Model not found:', model)
            return {
              success: false,
              message: message,
              reply: null,
              error: `ä¸æ”¯æŒçš„æ¨¡å‹: ${model}`,
              usage: null
            }
          }

          console.log('ğŸ“¦ Using model config:', modelConfig)

          let result
          if (modelConfig.provider === 'openai') {
            const apiKey = env?.OPENAI_API_KEY
            console.log('ğŸ” OpenAI Key check:', {
              exists: !!apiKey,
              type: typeof apiKey,
              length: apiKey ? apiKey.length : 0,
              startsWithSk: apiKey ? apiKey.startsWith('sk-') : false
            })
            
            if (!apiKey) {
              return {
                success: false,
                message: message,
                reply: null,
                error: 'OpenAI API Key æœªé…ç½®ã€‚è¯·æ£€æŸ¥Workersç¯å¢ƒå˜é‡OPENAI_API_KEY',
                usage: null
              }
            }
            result = await callOpenAI(message, model, apiKey, optimizedParams.temperature, optimizedParams.maxTokens)
          } else if (modelConfig.provider === 'deepseek') {
            const apiKey = env?.DEEPSEEK_API_KEY
            console.log('ğŸ” DeepSeek Key check:', {
              exists: !!apiKey,
              type: typeof apiKey,
              length: apiKey ? apiKey.length : 0,
              startsWithSk: apiKey ? apiKey.startsWith('sk-') : false
            })
            
            if (!apiKey) {
              return {
                success: false,
                message: message,
                reply: null,
                error: 'DeepSeek API Key æœªé…ç½®ã€‚è¯·æ£€æŸ¥Workersç¯å¢ƒå˜é‡DEEPSEEK_API_KEY',
                usage: null
              }
            }
            result = await callDeepSeek(message, model, apiKey, optimizedParams.temperature, optimizedParams.maxTokens)
          } else {
            console.error('âŒ Unsupported provider:', modelConfig.provider)
            return {
              success: false,
              message: message,
              reply: null,
              error: `ä¸æ”¯æŒçš„æä¾›å•†: ${modelConfig.provider}`,
              usage: null
            }
          }

          console.log('ğŸ‰ API call completed successfully')
          return {
            success: true,
            message: message,
            reply: result.reply,
            error: null,
            usage: result.usage
          }
        } catch (error) {
          console.error('ğŸ’¥ SendMessage mutation error:', error)
          return {
            success: false,
            message: args.input.message,
            reply: null,
            error: error.message || 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯',
            usage: null
          }
        }
      }
    }
  }
})

// æ ¹æ®æ¨¡å‹ä¼˜åŒ–å‚æ•° - å¹³è¡¡é€Ÿåº¦ä¸è´¨é‡
function getOptimizedParams(model, temperature, maxTokens) {
  // DeepSeek å¹³è¡¡ä¼˜åŒ–å‚æ•°
  if (model.includes('deepseek')) {
    return {
      temperature: temperature !== undefined ? temperature : 0.4, // é€‚ä¸­çš„éšæœºæ€§
      maxTokens: maxTokens !== undefined ? maxTokens : 800 // é€‚ä¸­çš„è¾“å‡ºé•¿åº¦
    }
  }
  
  // OpenAI é»˜è®¤å‚æ•°
  return {
    temperature: temperature !== undefined ? temperature : 0.7,
    maxTokens: maxTokens !== undefined ? maxTokens : 1000
  }
}

// AI æ¨¡å‹é…ç½®
const AI_MODELS = [
  {
    id: 'deepseek-chat',
    name: 'DeepSeek Chat',
    provider: 'deepseek',
    description: 'DeepSeekçš„å¯¹è¯æ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)'
  },
  {
    id: 'deepseek-coder',
    name: 'DeepSeek Coder',
    provider: 'deepseek',
    description: 'DeepSeekçš„ä»£ç ç”Ÿæˆæ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)'
  },
  {
    id: 'gpt-3.5-turbo',
    name: 'GPT-3.5 Turbo',
    provider: 'openai',
    description: 'OpenAIçš„å¿«é€Ÿå“åº”æ¨¡å‹'
  },
  {
    id: 'gpt-4',
    name: 'GPT-4',
    provider: 'openai', 
    description: 'OpenAIçš„æœ€å¼ºæ¨¡å‹'
  }
]

// OpenAI API è°ƒç”¨
async function callOpenAI(message, model, apiKey, temperature = 0.7, maxTokens = 1000) {
  console.log('ğŸ¤– Calling OpenAI API:', { model, messageLength: message.length, temperature, maxTokens })
  
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: message }],
      temperature: temperature,
      max_tokens: maxTokens
    })
  })

  const data = await response.json()
  
  if (!response.ok) {
    console.error('âŒ OpenAI API Error:', data)
    throw new Error(data.error?.message || `OpenAI API error: ${response.status}`)
  }

  console.log('âœ… OpenAI API Success')
  return {
    reply: data.choices[0].message.content,
    usage: {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens
    }
  }
}

// DeepSeek API è°ƒç”¨ - å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬
async function callDeepSeek(message, model, apiKey, temperature = 0.4, maxTokens = 800) {
  console.log('ğŸ§  Calling DeepSeek API (Balanced Mode):', { 
    model, 
    messageLength: message.length, 
    temperature, 
    maxTokens 
  })
  
  // å¹³è¡¡çš„è¯·æ±‚ä½“ - é€Ÿåº¦ä¸è´¨é‡å¹¶é‡
  const requestBody = {
    model: model,
    messages: [
      {
        role: 'system',
        content: 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼Œä¿æŒå›ç­”ç®€æ´ä½†ä¿¡æ¯å®Œæ•´ã€‚é¿å…è¿‡åº¦å†—é•¿çš„è§£é‡Šã€‚'
      },
      { 
        role: 'user', 
        content: message 
      }
    ],
    temperature: temperature,
    max_tokens: maxTokens,
    // å¹³è¡¡ä¼˜åŒ–å‚æ•°
    top_p: 0.8, // ä¿æŒä¸€å®šå¤šæ ·æ€§
    frequency_penalty: 0.1, // è½»å¾®å‡å°‘é‡å¤
    presence_penalty: 0.1, // è½»å¾®é¿å…å†—é•¿
    stop: ['\n\n\n', '###'], // é€‚åº¦çš„åœæ­¢æ¡ä»¶
    stream: false
  }

  console.log('ğŸ”§ DeepSeek balanced request params:', requestBody)
  
  const response = await fetch('https://api.deepseek.com/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(requestBody)
  })

  const data = await response.json()
  
  if (!response.ok) {
    console.error('âŒ DeepSeek API Error:', data)
    throw new Error(data.error?.message || `DeepSeek API error: ${response.status}`)
  }

  console.log('âœ… DeepSeek API Success (Balanced)')
  
  let reply = data.choices[0].message.content

  // è½»åº¦æ¸…ç†ï¼Œä¿ç•™å¤§éƒ¨åˆ†å†…å®¹
  reply = reply.replace(/<think>[\s\S]*?<\/think>/gi, '').trim()
  reply = reply.replace(/ã€æ€è€ƒã€‘[\s\S]*?ã€\/æ€è€ƒã€‘/gi, '').trim()
  
  // å¦‚æœå›å¤ä¸ºç©ºï¼Œæä¾›é»˜è®¤å›å¤
  if (!reply) {
    reply = 'æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œä½†ä¼¼ä¹å›å¤å†…å®¹å‡ºç°äº†é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚'
  }

  return {
    reply: reply,
    usage: {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens
    }
  }
}

// åˆ›å»º GraphQL Yoga å®ä¾‹
const yoga = createYoga({
  schema,
  context: async ({ request, env }) => {
    console.log('ğŸŒ Creating GraphQL context for:', request.method)
    console.log('ğŸ”§ Environment passed to context:', {
      envType: typeof env,
      envKeys: Object.keys(env || {}),
      hasOpenAI: !!env?.OPENAI_API_KEY,
      hasDeepSeek: !!env?.DEEPSEEK_API_KEY
    })
    return { request, env }
  },
  cors: {
    origin: [
      'https://zy-yd-ai-tools.pages.dev',
      'https://2000zy.space',
      'http://localhost:3000'
    ],
    methods: ['GET', 'POST', 'OPTIONS'],
    allowedHeaders: ['Content-Type', 'Authorization', 'Accept'],
    credentials: false
  },
  graphiql: {
    title: 'ZY-YD AI API - GraphQL Playground',
    headerEditorEnabled: true
  }
})

export default {
  async fetch(request, env, ctx) {
    const url = new URL(request.url)
    console.log('ğŸŒ Workers request:', {
      method: request.method,
      pathname: url.pathname,
      origin: request.headers.get('origin')
    })
    
    console.log('ğŸ”§ Environment at fetch level:', {
      envType: typeof env,
      envKeys: Object.keys(env || {}),
      hasOpenAI: !!env?.OPENAI_API_KEY,
      hasDeepSeek: !!env?.DEEPSEEK_API_KEY
    })
    
    try {
      const response = await yoga.fetch(request, { env, ctx })
      console.log('âœ… Workers response:', response.status)
      return response
    } catch (error) {
      console.error('ğŸ’¥ Workers error:', error)
      return new Response(
        JSON.stringify({ 
          error: 'Workers Internal Error', 
          message: error.message,
          stack: error.stack?.split('\n').slice(0, 5)
        }), 
        { 
          status: 500, 
          headers: { 
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization'
          } 
        }
      )
    }
  }
}